# -*- coding: utf-8 -*-
"""CNN model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n0sKGW6igW0KXCMiRbMwB8-TVamWJMjc
"""

import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms
import torchvision.datasets as dsets
import matplotlib.pyplot as plt
import math
import numpy as np
batch_size = 100 #Batchï¼šhow many photos per group
num_epochs = 10
learning_rate = 0.3
dimension = 28
dropout = [0.0, 0.5, 0.7, 0.9, 0.99, 0.999]
# constant for classes
classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')
accu_table = []
# if torch.cuda.is_available():
#     device = torch.device('cuda')
#     print("gpu")

# Training, augmented and testing datasets
train_augment_transforms = transforms.Compose([
    transforms.RandomHorizontalFlip(0.5),
    transforms.RandomAffine(degrees=10, translate=(0.1, 0.1)),
    transforms.ToTensor()
])

train_dataset = dsets.FashionMNIST(root='./data', 
                            train=True, 
                            transform=transforms.ToTensor(),
                            download=True)

train_aug_dataset = dsets.FashionMNIST(root='./data', 
                            train=True, 
                            transform=train_augment_transforms,
                            download=True)

test_dataset = dsets.FashionMNIST(root='./data', 
                           train=False, 
                           transform=transforms.ToTensor())
# print(train_dataset.train_data.size())
# print(test_dataset.test_data.size())


train_loader = torch.utils.data.DataLoader(dataset=train_dataset, 
                                          batch_size=batch_size, 
                                          shuffle=True)

train_aug_loader = torch.utils.data.DataLoader(dataset=train_dataset, 
                                                batch_size=batch_size, 
                                                shuffle=True)

test_loader = torch.utils.data.DataLoader(dataset=test_dataset, 
                                          batch_size=batch_size, 
                                          shuffle=False)
# print(len(train_loader))
# print(len(test_loader))
# for i, (images, labels) in enumerate(train_loader):
#   print(len(images))
#   break

class CNNModel(nn.Module):
    def __init__(self, width, drop):
        super(CNNModel, self).__init__()

        # Convolution 1
        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)
        self.relu1 = nn.ReLU()

        # Convolution 2
        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)
        self.relu2 = nn.ReLU()

        # Max pool 2
        self.maxpool2 = nn.MaxPool2d(kernel_size=2)

        # Fully connected 1 (readout)
        kernel = 3
        kernel_max2 = 2
        stride1,stride2 = 1,1
        width_cnn1 = math.ceil((width-kernel+2)/stride1 + 1 )
        # print("width_cnn1: ", width_cnn1)
        width_cnn2 = math.ceil((width_cnn1-kernel+2)/stride2 + 1 )
        # print("width_cnn2: ", width_cnn2)
        width_max2 = math.ceil((width_cnn2-kernel_max2)/kernel_max2 + 1)
        # print("width_max2: ", width_max2)
        self.fc1 = nn.Linear(32 * width_max2 * width_max2, 128)
        self.relu3 = nn.ReLU() 
        self.dropout = nn.Dropout(p=drop)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        # Convolution 1
        out = self.cnn1(x)
        out = self.relu1(out)
        # Convolution 2 
        out = self.cnn2(out)
        out = self.relu2(out)
        # Max pool 2 
        out = self.maxpool2(out)
        # Resize
        # Flattern the convolution output
        out = out.view(out.size(0), -1)
        # Linear function (readout)
        out = self.fc1(out)
        out = self.relu3(out)
        #dropout
        out = self.dropout(out)
        #linear func
        out = self.fc2(out)
        return out

# print(len(list(model.parameters())))
# for n in range(len(list(model.parameters()))):
#     print(list(model.parameters())[n].size())


# # Convolution 1: 16 Kernels
# print(list(model.parameters())[0].size())

# # Convolution 1 Bias: 16 Kernels
# print(list(model.parameters())[1].size())

# # Convolution 2: 32 Kernels with depth = 16
# print(list(model.parameters())[2].size())

# # Convolution 2 Bias: 32 Kernels with depth = 16
# print(list(model.parameters())[3].size())

# # Fully Connected Layer 1
# print(list(model.parameters())[4].size())

# # Fully Connected Layer Bias
# print(list(model.parameters())[5].size())

class Myclassifier:
  def __init__(self, dim, drop, rate):
    self.__model = CNNModel(dim, drop)
    self.__device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    self.__learning_rate = rate


  def Init(self):
    self.__model.to(self.__device)
    self.__criterion = nn.CrossEntropyLoss()
    self.__optimizer = torch.optim.SGD(self.__model.parameters(), lr=self.__learning_rate)
    self.__scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.__optimizer, 'min' )
    print("init ok")

  # def Clear(self):
  #   self.__model = None
  #   self.__device = None

  def Run(self, num):
    train_losses, test_losses = [], []
    running_loss = 0
    test_loss = 0
    for epoch in range(num_epochs):
      running_loss += self.RunTrainLoaders(train_loader)
      running_loss += self.RunTrainLoaders(train_aug_loader)
      total, correct, test_loss= self.RunTestLoaders(test_loader)
      accuracy = 100 * correct // total
      train_losses.append(running_loss/len(train_loader))
      # print("running_loss: ", running_loss)
      # print("len(train_loader): ", len(train_loader))
      test_losses.append(test_loss/len(test_loader))
      self.__scheduler.step(test_loss/len(test_loader))
      if epoch == 9:
        accu_table.append(accuracy)   
      # Print training and testing loss for each epoch
      print('training loss: {}. test loss: {}. Accuracy: {}'.format(running_loss/len(train_loader), test_loss/len(test_loader), accuracy))
      running_loss = 0
      self.__model.train()
    # plot loss for all 10 epochs
    plt.plot(train_losses, label='Training loss')
    plt.plot(test_losses, label='Validation loss')
    plt.title("chenhsi_model_"+str(num))
    plt.legend(frameon=False)
    plt.show()
    #save model
    torch.save(self.__model, "chenhsi_model_"+str(num))

  def RunTrainLoaders(self, loader):
    running_loss = 0
    for i, (images, labels) in enumerate(loader):
      # Load images
      images = images.requires_grad_().to(self.__device)
      labels = labels.to(self.__device)
      # Clear gradients w.r.t. parameters
      self.__optimizer.zero_grad()
      # Forward pass to get output/logits
      outputs = self.__model(images)
      # Calculate Loss: softmax --> cross entropy loss
      loss = criterion(outputs, labels)
      # Getting gradients w.r.t. parameters
      loss.backward()
      # Updating parameters
      self.__optimizer.step()
      running_loss += loss.item()
    return running_loss

  def RunTestLoaders(self, loader):
    # Calculate Accuracy
    test_loss = 0         
    correct = 0
    total = 0
    self.__model.eval()
    # Iterate through test dataset
    for images, labels in test_loader:
      # Load images
      images = images.requires_grad_().to(self.__device)
      labels = labels.to(self.__device)
      # Forward pass only to get logits/output
      outputs = self.__model(images)
      batch_loss = self.__criterion(outputs, labels)
      test_loss += batch_loss.item()
      # Get predictions from the maximum value
      _, predicted = torch.max(outputs.data, 1)
      # Total number of labels
      total += labels.size(0)
      # Total correct predictions
      correct += (predicted.cpu() == labels.cpu()).sum()
      return total, correct, test_loss

for num in dropout:
  myclassfier = Myclassifier(dimension, num, learning_rate)
  # model = CNNModel(28, num)
  # device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
  # model.to(device)
  myclassfier.Init()
  myclassfier.Run(num)
  # myclassfier.Clear()
  # criterion = nn.CrossEntropyLoss()
  # # Change learning rate
  # learning_rate = 0.3
  # # Try SGD, Adam.... etc 
  # optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
  # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min' ) 
  # # optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) 

  # train_losses, test_losses = [], []
  # running_loss = 0
  # test_loss = 0
  # for epoch in range(num_epochs):
  #   myclassfier.RunTestLoaders(train_loader, running_loss)
  #   myclassfier.RunTestLoaders(train_aug_loader, running_loss)
  #   total, correct = myclassfier.RunTestLoaders(test_loader, test_loss)
    # for i, (images, labels) in enumerate(train_loader):
    #   # Load images
    #   images = images.requires_grad_().to(device)
    #   labels = labels.to(device)
    #   # Clear gradients w.r.t. parameters
    #   optimizer.zero_grad()
    #   # Forward pass to get output/logits
    #   outputs = model(images)
    #   # Calculate Loss: softmax --> cross entropy loss
    #   loss = criterion(outputs, labels)
    #   # Getting gradients w.r.t. parameters
    #   loss.backward()
    #   # Updating parameters
    #   optimizer.step()
    #   running_loss += loss.item()
    # for i, (images, labels) in enumerate(train_aug_loader):
    #   # Load images
    #   images = images.requires_grad_().to(device)
    #   labels = labels.to(device)
    #   # Clear gradients w.r.t. parameters
    #   optimizer.zero_grad()
    #   # Forward pass to get output/logits
    #   outputs = model(images)
    #   # Calculate Loss: softmax --> cross entropy loss
    #   loss = criterion(outputs, labels)
    #   # Getting gradients w.r.t. parameters
    #   loss.backward()
    #   # Updating parameters
    #   optimizer.step()
    #   running_loss += loss.item()
    # # if iter % 600 == 0:
    # test_loss = 0
    # # Calculate Accuracy         
    # correct = 0
    # total = 0
    # model.eval()
    # # Iterate through test dataset
    # for images, labels in test_loader:
    #   # Load images
    #   images = images.requires_grad_().to(device)
    #   labels = labels.to(device)
    #   # Forward pass only to get logits/output
    #   outputs = model(images)
    #   batch_loss = criterion(outputs, labels)
    #   test_loss += batch_loss.item()
    #   # Get predictions from the maximum value
    #   _, predicted = torch.max(outputs.data, 1)
    #   # Total number of labels
    #   total += labels.size(0)
    #   # Total correct predictions
    #   correct += (predicted.cpu() == labels.cpu()).sum()
    # accuracy = 100 * correct // total
    # train_losses.append(running_loss/len(train_loader))
    # test_losses.append(test_loss/len(test_loader))
    # scheduler.step(test_loss/len(test_loader))
    # if epoch == 9:
    #   accu_table.append(accuracy)   
    # # Print training and testing loss for each epoch
    # print('training loss: {}. test loss: {}. Accuracy: {}'.format(running_loss/len(train_loader), test_loss/len(test_loader), accuracy))
    # # print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))

    # running_loss = 0
    # model.train()

  # # plot loss for all 10 epochs
  # plt.plot(train_losses, label='Training loss')
  # plt.plot(test_losses, label='Validation loss')
  # plt.title("chenhsi_model_"+str(num))
  # plt.legend(frameon=False)
  # plt.show()
  # #save model
  # torch.save(model, "chenhsi_model_"+str(num))
  # #model1 = torch.load("chenhsi_model_"+str(num))

#accuracy table
print(accu_table)
print(len(accu_table))
plt.plot(accu_table, 'r', label = 'validation accuracy')
plt.title("CNN Accuracy")
plt.legend(loc='lower left')

#Show 5 random photos with the best model
device_0 = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_0 = torch.load("chenhsi_model_0.0")
model_0.eval()

def predict_images(images):
    images = images.requires_grad_().to(device_0)
    outputs = model_0(images)
    _, predicted = torch.max(outputs.data, 1)
    return predicted

def get_random_images(num):
    count = 0
    test_loader_0 = torch.utils.data.DataLoader(dataset=test_dataset, 
                                                batch_size=num, 
                                                shuffle=False)
    target = np.random.randint(len(test_loader_0))
    print("random ind: ", target)
    for images, labels in test_loader_0:
      if target == count:
        return images, labels
      count += 1
  

to_pil = transforms.ToPILImage()
images, labels = get_random_images(5)
fig=plt.figure(figsize=(30,30))
predicted = predict_images(images)
print(predicted)
for ind in range(len(images)):
    image = to_pil(images[ind])
    sub = fig.add_subplot(1, len(images), ind+1)
    res = int(labels[ind]) == predicted[ind]
    sub.set_title(str(classes[labels[ind]]) + ":" + str(res))
    plt.axis('off')
    plt.imshow(image)
plt.show()

#Analysis:some bullet points(TBC)
#1.Changing learning rate from 0.01 to 0.3 increases the accuracy
#2.augmenting data increases accuracy
#3.Changing optimizer

# dropout = [0.0, 0.5, 0.7, 0.9, 0.99, 0.999]
# accu_table = []
# for num in dropout:
#   model = CNNModel(28, num)
#   device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
#   model.to(device)
#   criterion = nn.CrossEntropyLoss()
#   # Change learning rate
#   learning_rate = 0.3
#   # Try SGD, Adam.... etc 
#   optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
#   scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min' ) 
#   # optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) 
#   # iter = 0
#   train_losses, test_losses = [], []
#   running_loss = 0
#   for epoch in range(num_epochs):
#     for i, (images, labels) in enumerate(train_loader):
#       # Load images
#       images = images.requires_grad_().to(device)
#       labels = labels.to(device)
#       # Clear gradients w.r.t. parameters
#       optimizer.zero_grad()
#       # Forward pass to get output/logits
#       outputs = model(images)
#       # Calculate Loss: softmax --> cross entropy loss
#       loss = criterion(outputs, labels)
#       # Getting gradients w.r.t. parameters
#       loss.backward()
#       # Updating parameters
#       optimizer.step()

#       # iter += 1
#       running_loss += loss.item()
#     for i, (images, labels) in enumerate(train_aug_loader):
#       # Load images
#       images = images.requires_grad_().to(device)
#       labels = labels.to(device)
#       # Clear gradients w.r.t. parameters
#       optimizer.zero_grad()
#       # Forward pass to get output/logits
#       outputs = model(images)
#       # Calculate Loss: softmax --> cross entropy loss
#       loss = criterion(outputs, labels)
#       # Getting gradients w.r.t. parameters
#       loss.backward()
#       # Updating parameters
#       optimizer.step()
#       running_loss += loss.item()
#     # if iter % 600 == 0:
#     test_loss = 0
#     # Calculate Accuracy         
#     correct = 0
#     total = 0
#     model.eval()
#     # Iterate through test dataset
#     for images, labels in test_loader:
#       # Load images
#       images = images.requires_grad_().to(device)
#       labels = labels.to(device)
#       # Forward pass only to get logits/output
#       outputs = model(images)
#       batch_loss = criterion(outputs, labels)
#       test_loss += batch_loss.item()
#       # Get predictions from the maximum value
#       _, predicted = torch.max(outputs.data, 1)
#       # Total number of labels
#       total += labels.size(0)
#       # Total correct predictions
#       correct += (predicted.cpu() == labels.cpu()).sum()
#     accuracy = 100 * correct // total
#     train_losses.append(running_loss/len(train_loader))
#     test_losses.append(test_loss/len(test_loader))
#     scheduler.step(test_loss/len(test_loader))
#     if epoch == 9:
#       accu_table.append(accuracy)   
#     # Print training and testing loss for each epoch
#     print('training loss: {}. test loss: {}. Accuracy: {}'.format(running_loss/len(train_loader), test_loss/len(test_loader), accuracy))
#     # print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))

#     running_loss = 0
#     model.train()

#   # plot loss for all 10 epochs
#   plt.plot(train_losses, label='Training loss')
#   plt.plot(test_losses, label='Validation loss')
#   plt.title("chenhsi_model_"+str(num))
#   plt.legend(frameon=False)
#   plt.show()
#   #save model
#   torch.save(model, "chenhsi_model_"+str(num))
#   #model1 = torch.load("chenhsi_model_"+str(num))